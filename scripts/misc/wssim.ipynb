{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "import warnings\n",
        "\n",
        "!pip install fuzzywuzzy\n",
        "import fuzzywuzzy\n",
        "\n",
        "from fuzzywuzzy import fuzz\n",
        "import re\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "gXaxywfpfB3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv"
      ],
      "metadata": {
        "id": "8oHHrXCrWsEX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRk_2SE0e5lW"
      },
      "outputs": [],
      "source": [
        "!wget http://www.dianamccarthy.co.uk/downloads/WordMeaningAnno2012/cl-meaningincontext.tgz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzvf \"/content/cl-meaningincontext.tgz\" -C \"/content/\"\n"
      ],
      "metadata": {
        "id": "5XrYMIdCfoSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from html.parser import HTMLParser"
      ],
      "metadata": {
        "id": "jrzo_SM6C2s2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyHTMLParser(HTMLParser):\n",
        "\n",
        "    def __init__(self):\n",
        "        HTMLParser.__init__(self)\n",
        "        self.lemma2id2data = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: None)))\n",
        "        self.lemma = None\n",
        "        self.id_ = None\n",
        "        self.tag = None\n",
        "        self.endtag = None\n",
        "        self.sentence = None\n",
        "        self.preceding = None\n",
        "        self.following = None\n",
        "        self.target_id = None\n",
        "    \n",
        "    def handle_starttag(self, tag, attrs):\n",
        "        #print(\"Encountered a start tag:\", tag, attrs)\n",
        "        if tag == 'lexelt':\n",
        "            self.lemma = attrs[0][1]\n",
        "        if tag == 'instance':\n",
        "            self.id_ = attrs[0][1]\n",
        "        if tag == 'context':\n",
        "            self.sentence = ''\n",
        "        self.tag = tag\n",
        "\n",
        "    def handle_endtag(self, endtag):\n",
        "        #print(\"Encountered an end tag :\", endtag)\n",
        "        if endtag == 'instance':\n",
        "            lemma = self.lemma\n",
        "            id_ = self.id_\n",
        "            preceding = self.preceding if self.preceding!=None else ' '\n",
        "            following = self.following if self.following!=None else ' '\n",
        "            self.lemma2id2data[lemma][id_]['sentence'] = self.sentence.replace('\\n','').replace('\\t','').replace('    ','')\n",
        "            self.lemma2id2data[lemma][id_]['preceding'] = preceding.replace('\\n','').replace('\\t','').replace('    ','')\n",
        "            self.lemma2id2data[lemma][id_]['following'] = following.replace('\\n','').replace('\\t','').replace('    ','')\n",
        "            self.lemma2id2data[lemma][id_]['target_id'] = self.target_id        \n",
        "        self.endtag = endtag\n",
        "\n",
        "    def handle_data(self, data):\n",
        "        #print(\"Encountered some data  :\", data, self.tag, self.endtag)\n",
        "        if (self.tag == 'context' and (self.endtag == 'instance' or self.endtag == 'lexelt' or self.endtag == None)) or (self.tag == 'head' and self.endtag == 'head'):\n",
        "            self.sentence += data\n",
        "        if self.tag == 'head' and self.endtag == 'context':\n",
        "            self.following = data\n",
        "        if self.tag == 'head' and (self.endtag == 'instance' or self.endtag == 'lexelt' or self.endtag == None):\n",
        "            self.sentence += data\n",
        "            self.sentence = self.sentence.replace('\\n','').replace('\\t','').replace('    ','')\n",
        "            sentence_split = self.sentence.split()\n",
        "            self.target_id = len(sentence_split)-1\n",
        "        if self.tag == 'wcontext' and (self.endtag == 'instance' or self.endtag == 'lexelt' or self.endtag == None):\n",
        "            self.preceding = data"
      ],
      "metadata": {
        "id": "WZQltJPjDGXs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import xml.etree.ElementTree as ET\n",
        "with open('/content/Data/lexsub_wcdata.xml', encoding='iso-8859-1') as xmlfile:\n",
        "    data = xmlfile.read()\n",
        "    parser = MyHTMLParser()\n",
        "    parser.feed(data)\n",
        "    lemma2id2data = parser.lemma2id2data\n",
        "    "
      ],
      "metadata": {
        "id": "yGJ5S90qBYDR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs( '/content/wssim/', exist_ok=True )"
      ],
      "metadata": {
        "id": "96xqI4USFPo2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemma2id2context = defaultdict(lambda: defaultdict(lambda: None))\n",
        "for lemma, id2data in lemma2id2data.items():\n",
        "    lemma, pos = lemma.split('.')    \n",
        "    for id_, data in id2data.items():\n",
        "        identifier = lemma+'-'+id_\n",
        "        #print(identifier)\n",
        "        grouping = '1'\n",
        "        preceding = data['preceding'].strip(' ')\n",
        "        sentence = data['sentence']\n",
        "        following = data['following'].strip(' ')\n",
        "        leading_spaces = len(sentence) - len(sentence.lstrip(' '))\n",
        "        index = int(data['target_id']) - leading_spaces\n",
        "        sentence = sentence.strip(' ')\n",
        "        context = preceding + ' ' + sentence + ' ' + following\n",
        "        index = len(preceding.split()) + index\n",
        "        index_sentence = str(len(preceding.split()))+':'+str(len(preceding.split())+len(sentence.split()))\n",
        "        indd = str(len(preceding)) + ':' + str(len(preceding)+len(sentence)+1)\n",
        "        context = {'lemma':lemma, 'pos':pos, 'date': ' ', 'grouping':grouping, 'identifier':identifier, 'description':' ', 'context': context, 'indexes_target_token':' ', 'indexes_target_sentence':indd, 'context_tokenized':context, 'indexes_target_token_tokenized':index, 'indexes_target_sentence_tokenized':index_sentence}  \n",
        "        \n",
        "        lemma2id2context[lemma][lemma+'-'+id_] = context\n",
        "        \n",
        "with open('/content/Markup/WordSenseSimilarity/wssim2ratings.csv', encoding='utf-8') as csvfile: \n",
        "    reader = csv.DictReader(csvfile, delimiter=',',quoting=csv.QUOTE_NONE,strict=True)\n",
        "    table = [row for row in reader]\n",
        "\n",
        "lemma2data = defaultdict(lambda: [])\n",
        "for row in table:    \n",
        "    lemma, pos = row['lemma'].split('.')\n",
        "    id1 = row['lexsub_id']\n",
        "    id2 = row['sense_id']\n",
        "    comment = ' '\n",
        "    judgment = row['judgment']\n",
        "    annotator = row['user_id']\n",
        "    if annotator == 'avg':\n",
        "        continue\n",
        "    data = {'identifier':lemma+'-'+id1,'sense_id': id2,'annotator':annotator,'judgment':float(judgment),'comment':comment,'lemma':lemma}    \n",
        "    lemma2data[lemma].append(data)\n",
        "    \n",
        "for lemma in lemma2data:\n",
        "    output_folder = 'wssim'+'/' +lemma+'/'    \n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)    \n",
        "\n",
        "    # Export data\n",
        "    with open(output_folder +'judgments.csv', 'w') as f:  \n",
        "        w = csv.DictWriter(f, lemma2data[lemma][0].keys(), delimiter='\\t', quoting = csv.QUOTE_NONE, quotechar='')\n",
        "        w.writeheader()\n",
        "        w.writerows(lemma2data[lemma])\n",
        "\n",
        "    contexts = list(lemma2id2context[lemma].values())\n",
        "    \n",
        "    # Export data\n",
        "    with open(output_folder +'uses.csv', 'w') as f:  \n",
        "        w = csv.DictWriter(f, contexts[0].keys(), delimiter='\\t', quoting = csv.QUOTE_NONE, quotechar='')\n",
        "        w.writeheader()\n",
        "        w.writerows(contexts)\n",
        "        \n",
        "        "
      ],
      "metadata": {
        "id": "wGaSeqGNE3rf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dirs = os.listdir('wssim')"
      ],
      "metadata": {
        "id": "TESIK5sU_z7t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_u = []\n",
        "for i in dirs:\n",
        "  path_u.append(\"wssim/\" + i + \"/uses.csv\")"
      ],
      "metadata": {
        "id": "1ViezOzj_9sf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_j = []\n",
        "for i in dirs:\n",
        "  path_j.append(\"wssim/\" + i + \"/judgments.csv\")"
      ],
      "metadata": {
        "id": "llp6trN0AOlT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wssim_uses_df = pd.DataFrame()            #wssim uses df\n",
        "for i in path_u:\n",
        "   Tmp = pd.read_csv(i, delimiter='\\t', quoting = 3)\n",
        "   Tmp['dataset'] = i.split('/')[0]\n",
        "   wssim_uses_df = pd.concat([wssim_uses_df, Tmp])"
      ],
      "metadata": {
        "id": "iGEMKmjsAdOS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wssim_uses_df = wssim_uses_df.reset_index(drop = True)"
      ],
      "metadata": {
        "id": "wgn47IAFT3vP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_indice(sent, indice):\n",
        "  tag = ''\n",
        "  indice = int(indice)\n",
        "  tok = sent.split(\" \")\n",
        "  #print(tok)\n",
        "  for i in range(len(tok)):\n",
        "    if i == indice:\n",
        "      tag = tok[i]\n",
        "  return str(sent.find(tag))+\":\"+str(sent.find(tag)+len(tag))"
      ],
      "metadata": {
        "id": "IlMWs59LUPRP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wssim_uses_df[\"indexes_target_token\"] = wssim_uses_df.apply(lambda x: get_indice(x.context_tokenized, x.indexes_target_token_tokenized), axis=1)"
      ],
      "metadata": {
        "id": "tducTa2xUYg_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uses = wssim_uses_df"
      ],
      "metadata": {
        "id": "PXCueBDpaG3c"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Garrafao/WUGs.git"
      ],
      "metadata": {
        "id": "aBWMBMBeaO50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('/content/WUGs/scripts/misc/data/dwug_en/data/*/', exist_ok=True )"
      ],
      "metadata": {
        "id": "5-3vRsmVaCA7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "uses.to_csv('/content/WUGs/scripts/misc/data/dwug_en/data/*/uses.csv', sep='\\t', encoding='utf-8', quoting = csv.QUOTE_NONE, quotechar='')"
      ],
      "metadata": {
        "id": "zV-aVghSaTlD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = '/content/WUGs/scripts/misc/data/dwug_en/data/'"
      ],
      "metadata": {
        "id": "6twJtALldzfE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%run /content/WUGs/scripts/misc/use2normalize.py /content/WUGs/scripts/misc/data/dwug_en/data/*/uses.csv dwug_en output_file"
      ],
      "metadata": {
        "id": "jYSQ4PpfaZg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_uses = pd.read_csv('/content/output_file', delimiter = '\\t',quoting =3)"
      ],
      "metadata": {
        "id": "GRjI32zMebOu"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_uses['context_tokenized'] = wssim_uses_df['context_tokenized']\n",
        "final_uses['pos'] = wssim_uses_df['pos']\n",
        "final_uses['dataset'] = wssim_uses_df['dataset']\n",
        "final_uses['grouping'] = wssim_uses_df['grouping']\n",
        "final_uses['lemma'] = wssim_uses_df['lemma']\n",
        "final_uses['indexes_target_token_tokenized'] = wssim_uses_df['indexes_target_token_tokenized']\n",
        "final_uses['indexes_target_sentence_tokenized'] = wssim_uses_df['indexes_target_sentence_tokenized']\n",
        "final_uses['date'] = \" \"\n",
        "final_uses['description'] = \" \""
      ],
      "metadata": {
        "id": "hOGVzi54eo1b"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_uses = final_uses[['lemma', 'pos', 'date', 'grouping','identifier', 'description', 'context', 'indexes_target_token', 'indexes_target_sentence', 'context_tokenized', 'indexes_target_token_tokenized', 'indexes_target_sentence_tokenized' ,'dataset']]"
      ],
      "metadata": {
        "id": "INkCOilEemEi"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs( '/content/WSSim', exist_ok=True )"
      ],
      "metadata": {
        "id": "WnFPd24ViYzp"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in list(final_uses[\"lemma\"].value_counts().index):\n",
        "  df_temp = final_uses[final_uses[\"lemma\"]==i]\n",
        "  numpy_df = df_temp.to_numpy()\n",
        "  header = list(df_temp.columns)\n",
        "  numpy_df = np.vstack([header, numpy_df])\n",
        "  if not os.path.exists('/content/WSSim'+\"/\"+i):\n",
        "      os.mkdir('/content/WSSim'+\"/\"+i)\n",
        "  np.savetxt('/content/WSSim'+\"/\"+i+\"/uses.csv\", numpy_df,fmt='%s', delimiter='\\t')"
      ],
      "metadata": {
        "id": "bRekzSFriLaS"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wssim_judgemnt_df = pd.DataFrame()            #wssim judgments df\n",
        "for i in path_j:\n",
        "   Tmp = pd.read_csv(i, delimiter='\\t', quoting = 3)\n",
        "   Tmp['dataset'] = i.split('/')[0]\n",
        "   wssim_judgemnt_df = pd.concat([wssim_judgemnt_df, Tmp])\n"
      ],
      "metadata": {
        "id": "U_wWvIeXGVEG"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wssim_judgemnt_df.reset_index(drop = True)"
      ],
      "metadata": {
        "id": "oPq4Pl9zXPeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in list(wssim_judgemnt_df[\"lemma\"].value_counts().index):\n",
        "  df_temp = wssim_judgemnt_df[wssim_judgemnt_df[\"lemma\"]==i]\n",
        "  numpy_df = df_temp.to_numpy()\n",
        "  header = list(df_temp.columns)\n",
        "  numpy_df = np.vstack([header, numpy_df])\n",
        "  if not os.path.exists('/content/WSSim'+\"/\"+i):\n",
        "      os.mkdir('/content/WSSim'+\"/\"+i)\n",
        "  np.savetxt('/content/WSSim'+\"/\"+i+\"/judgments.csv\", numpy_df,fmt='%s', delimiter='\\t')"
      ],
      "metadata": {
        "id": "AYgdok8WBOl0"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rPHl4v2TjsOC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}