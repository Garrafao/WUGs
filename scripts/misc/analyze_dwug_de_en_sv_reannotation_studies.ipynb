{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "From December 2022 to July 2023 we conducted several annotation studies on the data from the DWUG DE/EN/SV data sets. These can be distinguished into basically three studies:\n",
    "- Study 1: Small number of randomly selected edges\n",
    "- Study 2: resample 25+25 uses and annotate them rather densely\n",
    "- Study 3: extra round with uncompared clusters\n",
    "\n",
    "## Study 1 ('uses')\n",
    "Study 1 has been done with DWUG DE/EN/SV. For Swedish it has been repeated because of low agreement. We uploaded all uses for all words from the most recent DWUG data sets to the DURel system and instructed each annotator to annotate each word for 30 minutes. One exception: in the second study with Swedish data annotators had 60 minutes for each word. The order of annotation instances (pairs) was randomized by the DURel tool. Some important points:\n",
    "\n",
    "- Annotators JoshuaC99 and Frida didn't finish the study and should be excluded if low agreement with other annotators.\n",
    "- In the Swedish studies some words were excluded from the annotation during the first annotation round because there seems to be many \"Cannot decide\" judgments. Hence, we decided to instruct annotators to skip those words which we had already excluded for SemEval. These are:\n",
    "\n",
    "anda\n",
    "bäck\n",
    "dun\n",
    "fack\n",
    "fången\n",
    "framlida\n",
    "gloria\n",
    "ingående\n",
    "jordisk\n",
    "mode\n",
    "sittning\n",
    "stöt\n",
    "uppslag\n",
    "\n",
    "It is possible that some of these words have few annotations because this instruction was sent out to annotators during the first round of annotation. See below for Swedish data quality. See below for Swedish data quality, agreement and multiple rounds for Swedish.\n",
    "\n",
    "\n",
    "## Study 2 ('resampled')\n",
    "For this study, we resampled (random without replacement) 25+25 uses for 15 words from the source corpora used for SemEval. Important: In resampling we did not apply the constraint we had in SemEval on sentence length. Excluded words from above were also excluded from the annotation. See below for Swedish data quality, agreement and multiple rounds for Swedish.\n",
    "\n",
    "## Study 3 ('unc')\n",
    "In this study, we took the aggregated graphs from the latest DWUG and DiscoWUG data sets and sampled (at most) 3 edges for clusters which had not been compared with each other (see cluster_find_uncompared_edges.ipynb). The aim is to make the annotation algorithm described in SemEval converge (all cluster combinations are annotated). All annotators annotated the same data (all sampled edges). See below for Swedish data quality and agreement.\n",
    "\n",
    "### General notes\n",
    "- Swedish data shows consistently low agreement. The data is also very dirty (many OCR errors). This is one explanation.\n",
    "- Because of low agreement, study 1 and 2 have been repeated on Swedish with other annotators.\n",
    "- During the annotation we observed several bugs in the annotation system. These were subsequently solved but may have had minor influences on the annotation:\n",
    " * In the first round of study 2 some of the indices of target words were erroneous (mostly off by small number of characters). This led to a bug in the annotation system showing annotators a message that everything was annotated. After the bug was solved annotators were instructed to continue the annotation.\n",
    " * In June we discovered some bugs concerning randomization:\n",
    "     * randomization within pairs: the randomization of order within annotation pairs did not work for projects uploaded with the \"upload pairs\" function (concerns only study 2) and the tutorials. This was solved in early July, so it (probably) did not affect dwug_sv_resampled2. This was (probably) not present in the first round of studies.\n",
    "     * randomization of sequence: the underlying annotation sequence was not fixed, so some pairs were annotated multiple times and sometimes not all pairs uploaded with the \"upload pairs\" function were annotated. This was solved in early July, so it did not affect dwug_sv_resampled2, but maybe dwug_sv_uses2. For the first round of studies this was solved with a hot fix.\n",
    "     * skipped instances: the annotation index was increased twice when clicking once. This (probably) had no effect on the annotated data. Couldn't be reproduced, so no fix was applied.\n",
    "- todo Dominik: validate the statistics calculated in this notebook with previous calculations on earlier data versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    "- install packages as described in the readme of this repository\n",
    "- if you observe any problems, please open an issue in this repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scripts from parent folder\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download re-annotated data\n",
    "import requests\n",
    "data = 'https://drive.google.com/uc?export=download&id=13FS46AVAFnEwx1NwJAYgbd0DbozmrS4q'\n",
    "r = requests.get(data, allow_redirects=True)\n",
    "f = 'dwug_de_en_sv_reannotation_studies-from-annotation-workspace.zip'\n",
    "open(f, 'wb').write(r.content)\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(f) as z:\n",
    "    z.extractall()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "# Preprocess annotated data\n",
    "datasets = ['discowug_unc', 'dwug_de_resampled', 'dwug_de_unc', 'dwug_de_uses', 'dwug_en_resampled', 'dwug_en_uses', 'dwug_sv_resampled', 'dwug_sv_resampled2', 'dwug_sv_unc', 'dwug_sv_uses', 'dwug_sv_uses2']\n",
    "\n",
    "# Make output directory\n",
    "input_path = 'dwug_de_en_sv_reannotation_studies-from-annotation-workspace'\n",
    "output_path = 'data_output'\n",
    "Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "utility = ['random', 'xlmr+mlp+binary', 'tuozhang', 'garrafao']\n",
    "low_agreement = ['Frida', 'JoshuaC99'] + ['Tegehall', 'Maria Rumar']\n",
    "undesired_annotators = utility + low_agreement  # They need to be filtered out\n",
    "\n",
    "# map identifiers of uses (data still has DURel-internal identifiers)\n",
    "for dataset in datasets:\n",
    "    for p in Path(input_path+'/'+dataset+'/data').glob('*/'):\n",
    "        print(p)\n",
    "        lemma = str(p).split('/')[-1]\n",
    "        with open(str(p)+'/'+'uses.csv', encoding='utf-8') as csvfile: \n",
    "            reader = csv.DictReader(csvfile, delimiter='\\t',quoting=csv.QUOTE_NONE,strict=True)\n",
    "            uses = [row for row in reader]\n",
    "        with open(str(p)+'/'+'judgments.csv', encoding='utf-8') as csvfile: \n",
    "            reader = csv.DictReader(csvfile, delimiter='\\t',quoting=csv.QUOTE_NONE,strict=True)\n",
    "            judgments = [row for row in reader]\n",
    "\n",
    "        # Get mapping of system identifiers to original identifiers\n",
    "        identifiersystem2identifier = {row['identifier_system']:row['identifier'] for row in uses}\n",
    "        \n",
    "        uses_out = []\n",
    "        for row in uses:\n",
    "            row_out = {key:val for key, val in row.items() if not key in ['identifier_system','project','lang','user']}\n",
    "            uses_out.append(row_out)\n",
    "        \n",
    "        judgments_out = []\n",
    "        for row in judgments:\n",
    "            if row['annotator'] in undesired_annotators: # filter out undesired annotators\n",
    "                continue\n",
    "            row_out = {key:(val if not key in ['identifier1','identifier2'] else identifiersystem2identifier[val]) for key, val in row.items()}            \n",
    "            judgments_out.append(row_out)\n",
    "\n",
    "        # Continue if word was not annotated    \n",
    "        if judgments_out == []:\n",
    "            continue        \n",
    "        \n",
    "        output_path_lemma = output_path+'/'+dataset+'/data/'+lemma+'/'\n",
    "        Path(output_path_lemma).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(output_path_lemma+'/'+'judgments.csv', 'w') as f:  \n",
    "                w = csv.DictWriter(f, judgments_out[0].keys(), delimiter='\\t', quoting = csv.QUOTE_NONE, quotechar='')\n",
    "                w.writeheader()\n",
    "                w.writerows(judgments_out)\n",
    "        \n",
    "        with open(output_path_lemma+'/'+'uses.csv', 'w') as f:  \n",
    "                w = csv.DictWriter(f, uses_out[0].keys(), delimiter='\\t', quoting = csv.QUOTE_NONE, quotechar='')\n",
    "                w.writeheader()\n",
    "                w.writerows(uses_out)\n",
    "                \n",
    "                \n",
    "# todo: clean the comment column\n",
    "# todo: rename lemma folders in English data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download original data for comparison\n",
    "path_original = 'datasets_original'\n",
    "datasets_original = ['dwug_de', 'dwug_en', 'dwug_sv', 'discowug'] # versions: 'dwug_de_230', 'dwug_en_201', 'dwug_sv_201', 'discowug_111'\n",
    "datasets_original_links = ['https://zenodo.org/record/7441645/files/dwug_de.zip?download=1', 'https://zenodo.org/record/7387261/files/dwug_en.zip?download=1', 'https://zenodo.org/record/7389506/files/dwug_sv.zip?download=1', 'https://zenodo.org/record/7396225/files/discowug.zip?download=1']\n",
    "\n",
    "Path(path_original).mkdir(parents=True, exist_ok=True)\n",
    "for i in range(len(datasets_original)):\n",
    "    name = datasets_original[i] \n",
    "    link = datasets_original_links[i] \n",
    "    r = requests.get(link, allow_redirects=True)\n",
    "    f = path_original+'/'+name+'.zip'\n",
    "    open(f, 'wb').write(r.content)\n",
    "\n",
    "    with zipfile.ZipFile(f) as z:\n",
    "        z.extractall(path_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load new datasets into data frame\n",
    "df_judgments = pd.DataFrame()\n",
    "for dataset in datasets:\n",
    "    for p in Path(output_path+'/'+dataset+'/data').glob('*/judgments.csv'):\n",
    "        #print(p)\n",
    "        df = pd.read_csv(p, delimiter='\\t', quoting=3, na_filter=False)\n",
    "        df['dataset'] = dataset\n",
    "        df_judgments = pd.concat([df_judgments, df])\n",
    "display(df_judgments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load old/original datasets into data frame\n",
    "df_judgments_original = pd.DataFrame()\n",
    "for dataset in datasets_original:\n",
    "    for p in Path(path_original+'/'+dataset+'/data').glob('*/judgments.csv'):\n",
    "        #print(p)\n",
    "        df = pd.read_csv(p, delimiter='\\t', quoting=3, na_filter=False)\n",
    "        df['dataset'] = dataset\n",
    "        df_judgments_original = pd.concat([df_judgments_original, df])\n",
    "display(df_judgments_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data frames and validate\n",
    "import numpy as np\n",
    "# Get all annotators\n",
    "annotators = df_judgments.annotator.unique()\n",
    "#display(annotators)\n",
    "\n",
    "# Get mapping from studies to annotators\n",
    "dataset2annotators = df_judgments.groupby(['dataset']).annotator.unique()\n",
    "#display(dataset2annotators)\n",
    "#display(dataset2annotators['dwug_de_resampled'])\n",
    "\n",
    "# Get aggregated data as instance versus annotator\n",
    "df_judgments[['identifier1','identifier2']] = np.sort(df_judgments[['identifier1','identifier2']], axis=1) # sort within pairs to be able to aggregate\n",
    "df_judgments_pair_vs_ann = pd.DataFrame()\n",
    "for annotator in annotators:\n",
    "    judgments_annotator = df_judgments[df_judgments['annotator'] == annotator][['identifier1', 'identifier2', 'lemma', 'dataset', 'judgment']].rename(columns={'judgment': annotator}, inplace=False)\n",
    "    #display(judgments_annotator)\n",
    "    df_judgments_pair_vs_ann = pd.concat([df_judgments_pair_vs_ann, judgments_annotator])\n",
    "\n",
    "#display(df_judgments_pair_vs_ann)\n",
    "df_judgments_pair_vs_ann_aggregated = df_judgments_pair_vs_ann.groupby(['identifier1','identifier2']).first().reset_index()  \n",
    "display(df_judgments_pair_vs_ann_aggregated)\n",
    "# Sanity check number of rows\n",
    "df_judgments_aggregated = df_judgments.groupby(['identifier1','identifier2']).agg({'judgment':lambda x: list(x)})\n",
    "#display(df_judgments_aggregated)\n",
    "assert len(df_judgments_aggregated.index) == len(df_judgments_pair_vs_ann_aggregated.index)\n",
    "# Check for duplicate rows in non-aggregated (but sorted) data frame\n",
    "assert len(df_judgments.index) == len(df_judgments.drop_duplicates().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate agreement for each study (new data)\n",
    "import krippendorff_ as krippendorff\n",
    "from itertools import combinations\n",
    "\n",
    "gb = df_judgments_pair_vs_ann_aggregated.groupby('dataset')\n",
    "groups = gb.groups\n",
    "for dataset in groups.keys():\n",
    "    \n",
    "    df_group = gb.get_group(dataset)\n",
    "    annotators_dataset = dataset2annotators[dataset]\n",
    "    df_group = df_group[annotators_dataset]\n",
    "    df_group = df_group.replace('0.0', np.nan) # 0.0 judgments mean \"cannot decide\"\n",
    "\n",
    "    data = np.transpose(df_group.values)\n",
    "    kri = krippendorff.alpha(reliability_data=data, level_of_measurement='ordinal')\n",
    "    #print(data)\n",
    "    print(dataset, kri)\n",
    "\n",
    "    # Pairwise\n",
    "    for a, b in combinations(annotators_dataset, 2):\n",
    "        data = [df_group[a].values, df_group[b].values]\n",
    "        kri = krippendorff.alpha(reliability_data=data, level_of_measurement='ordinal')\n",
    "        print('  ', a, b, kri)\n",
    "        \n",
    "# Per lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate annotators for exclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make annotator names unique across all datasets\n",
    "df_judgments_original['annotator'] = df_judgments_original.annotator + '-' + df_judgments_original.dataset\n",
    "df_judgments_all = pd.concat([df_judgments, df_judgments_original], axis=0)\n",
    "annotators = df_judgments_all.annotator.unique()\n",
    "info = df_judgments_all[['annotator', 'dataset']].drop_duplicates()\n",
    "info['lang'] = info.dataset.apply(lambda x: 'en' if '_en' in x else 'sv' if '_sv' in x else 'de')\n",
    "dataset_annotators = info.groupby('dataset')['annotator'].apply(lambda x: list(set(x)))\n",
    "dataset_language = info[['dataset', 'lang']].drop_duplicates().set_index('dataset')['lang']\n",
    "language_annotators = info[['annotator', 'lang']].drop_duplicates().groupby('lang').annotator.apply(list)\n",
    "annotator_language = info[['annotator', 'lang']].drop_duplicates().set_index('annotator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for duplicate judgments (same item, same annotator), take the median (86 total, mostly pairs)\n",
    "# unless the median is out of domain, in which case ignore\n",
    "value_domain = [float(v) for v in range(1, 4+1)]\n",
    "index_cols = ['identifier1' ,'identifier2', 'annotator', 'lemma', 'dataset']\n",
    "display(df_judgments_all[df_judgments_all.duplicated(subset=index_cols, keep=False)])\n",
    "df_judgments_all = df_judgments_all.groupby(index_cols).median('judgment').reset_index()\n",
    "df_judgments_all = df_judgments_all[df_judgments_all.judgment.apply(lambda x: x in value_domain)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get aggregated data as instance versus annotator\n",
    "df_judgments_all[['identifier1','identifier2']] = np.sort(df_judgments_all[['identifier1','identifier2']], axis=1) # sort within pairs to be able to aggregate\n",
    "df_judgments_all_pair_vs_ann = pd.DataFrame()\n",
    "annotator2dataset = df_judgments_all[['annotator', 'dataset']].drop_duplicates().set_index('annotator')\n",
    "for annotator in annotators:\n",
    "    judgments_annotator = df_judgments_all[df_judgments_all['annotator'] == annotator][['identifier1', 'identifier2', 'lemma', 'dataset', 'judgment']].rename(columns={'judgment': annotator}, inplace=False)\n",
    "    #display(judgments_annotator)\n",
    "    df_judgments_all_pair_vs_ann = pd.concat([df_judgments_all_pair_vs_ann, judgments_annotator])\n",
    "\n",
    "#display(df_judgments_pair_vs_ann)\n",
    "df_judgments_all_pair_vs_ann_aggregated = df_judgments_all_pair_vs_ann.groupby(['identifier1','identifier2']).first().reset_index()  \n",
    "# display(df_judgments_all_pair_vs_ann_aggregated)\n",
    "# Sanity check number of rows\n",
    "df_judgments_all_aggregated = df_judgments_all.groupby(['identifier1','identifier2']).agg({'judgment':lambda x: list(x)})\n",
    "#display(df_judgments_aggregated)\n",
    "assert len(df_judgments_all_aggregated.index) == len(df_judgments_all_pair_vs_ann_aggregated.index)\n",
    "# Check for duplicate rows in non-aggregated (but sorted) data frame (we cheated so there definitely aren't...)\n",
    "assert len(df_judgments_all.index) == len(df_judgments_all.drop_duplicates().index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import get_agreements\n",
    "value_domain = [float(v) for v in range(int(1), int(4)+1)]\n",
    "level_of_measurement = 'ordinal'\n",
    "df = df_judgments_all_pair_vs_ann_aggregated.copy()\n",
    "df = df.replace(0, np.nan)\n",
    "df = df[annotators] \n",
    "stats = get_agreements(df, value_domain=value_domain, level_of_measurement=level_of_measurement, metrics=['spr', 'kri'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_lang = {}\n",
    "for lang in language_annotators.keys():\n",
    "    df_lang = df[language_annotators.loc[lang]].dropna(axis=0, how='all')\n",
    "    stats_lang[lang] = get_agreements(df_lang, value_domain=value_domain, level_of_measurement=level_of_measurement, metrics=['spr', 'kri'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric, agg in [('kri', 'full'), ('spr', 'mean_weighted')]:\n",
    "    print(metric, agg)\n",
    "    for lang in language_annotators.keys():\n",
    "        print(lang, stats_lang[lang][metric][agg])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise permutations (per-annotator block analysis)\n",
    "annotator_agg = {a: {'krippendorff': [], 'spearman': [], 'support': []} for a in annotators}\n",
    "lang_agg = {l: {'krippendorff': [], 'spearman': [], 'support': []} for l in ['de', 'en', 'sv']}\n",
    "for a, b in permutations(annotators, 2):\n",
    "    data = [df[a].values, df[b].values]\n",
    "    n_common = (~np.isnan(data[0]) * ~np.isnan(data[1])).sum()\n",
    "    if n_common == 0:\n",
    "        continue\n",
    "    lang = annotator_language.loc[a].item()\n",
    "    assert(lang == annotator_language.loc[b].item())\n",
    "    kri = krippendorff.alpha(reliability_data=data, level_of_measurement='ordinal')\n",
    "    if n_common >= 3:\n",
    "        spr, spr_p = spearmanr(data[0], data[1], nan_policy='omit')\n",
    "    else:\n",
    "        spr, spr_p = np.nan, np.nan\n",
    "    # only record for 'a' since we duplicate with roles switched (permutations)\n",
    "    annotator_agg[a]['krippendorff'].append(kri)\n",
    "    annotator_agg[a]['spearman'].append(spr)\n",
    "    annotator_agg[a]['support'].append(n_common)\n",
    "    lang_agg[lang]['krippendorff'].append(kri)\n",
    "    lang_agg[lang]['spearman'].append(spr)\n",
    "    lang_agg[lang]['support'].append(n_common)\n",
    "    print(f'  {a:<20} {b:<20} {n_common: 5d} {kri: 0.3f} {spr: 0.3f}(p={spr_p:0.3f})') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in ['de', 'sv', 'en']:\n",
    "    for metric in ['krippendorff', 'spearman', 'support']:\n",
    "        print(lang, metric, np.nanmean(np.array(lang_agg[lang][metric])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unweighted summaries of pairwise statistics\n",
    "stats = pd.DataFrame.from_dict(annotator_agg, orient=\"index\").stack().to_frame()\n",
    "stats = pd.DataFrame(stats[0].values.tolist(), index=stats.index).T\n",
    "summary = stats.describe()\n",
    "annotators_ordered = summary.xs('krippendorff', level=1, axis=1).sort_values('mean', axis=1).columns\n",
    "for a in annotators_ordered:\n",
    "    print(a)\n",
    "    display(summary[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of pairwise scores, weighted by number of items annotated by both in the pair\n",
    "weights = stats.xs('support', level=1, axis=1) / stats.xs('support', level=1, axis=1).sum(axis=0)\n",
    "stats = stats.drop('support', axis=1, level=1).mul(weights, level=0).sum().unstack().sort_values('krippendorff')\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotator weighted pairwise agreements per dataset / per language \n",
    "import plotly.express as px\n",
    "df = stats.unstack().reset_index().rename({'level_0': 'metric', 'level_1': 'annotator', 0: 'value'}, axis=1).set_index('annotator')\n",
    "df['dataset'] = False\n",
    "for dataset in dataset_annotators.index:  \n",
    "    dataset_df = df.loc[dataset_annotators.loc[dataset]].copy()\n",
    "    dataset_df['slice']='dataset'\n",
    "    lang_df = df.loc[info[info.lang == dataset_language.loc[dataset]].annotator].copy()\n",
    "    lang_df['slice']='language'\n",
    "    tmp = pd.concat([dataset_df.reset_index(), lang_df.drop_duplicates().reset_index()], axis=0)\n",
    "    fig = px.box(tmp, y='value', x='slice', color='metric', points='all', height=600,  hover_name=tmp.annotator, title=dataset)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect some particular pairs of annotators\n",
    "a1, a2 = 'JoshuaC99', 'annotator8-dwug_en'\n",
    "wecare = df_judgments_all_pair_vs_ann_aggregated.apply(lambda x: ~np.isnan(x[a1]) and ~np.isnan(x[a2]), axis=1)\n",
    "df_judgments_all_pair_vs_ann_aggregated[wecare][['identifier1', 'identifier2', 'lemma', 'dataset', a1, a2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run WUG pipeline\n",
    "An example how to run WUG pipeline. Check readme of this repository to see what the pipeline can do. Many parameters can currently only be modified by defining your own parameterfile and providing it as input parameter to the pipeline (see below).\n",
    "Form:\n",
    "\n",
    "`bash -e scripts/run_system2.sh $dir $algorithm $position $parameterfile`\n",
    "\n",
    "Output is written to `$dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash -e ../run_system2.sh data_output/dwug_en_resampled correlation spring ../parameters_system2.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate agreement when merging studies\n",
    "# Check whether 'unc' data leads to more connected clusters\n",
    "# Compare clusterings on 'uses' data to clustering on previous data\n",
    "# Compare change scores on 'resampled' studies to previous data\n",
    "# Compare agreement across time periods (use grouping information from uses files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocessing\n",
    "## filter out bad annotators (low agreement)\n",
    "## export filtered data again\n",
    "## prepare data for publishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
