{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e76464b9-6979-45ea-80e0-edb634ca0bae",
   "metadata": {},
   "source": [
    "## Download data from Zenodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d675b9b-cc4d-45be-bf0c-934067019882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "datasets_to_download_zenodo = [('refwug', 'German', '1.1.0', 'https://zenodo.org/records/5791269/files/refwug.zip?download=1', ''), \n",
    "                            ('durel', 'German', '3.0.0', 'https://zenodo.org/records/5784453/files/durel.zip?download=1', ''),\n",
    "                            ('surel', 'German', '3.0.0', 'https://zenodo.org/records/5784569/files/surel.zip?download=1', ''), \n",
    "                            ('chiwug', 'Chinese', '1.0.0', 'https://zenodo.org/records/10023263/files/chiwug.zip?download=1', ''),\n",
    "                            ('dwug_de', 'German', '3.0.0', 'https://zenodo.org/records/14028509/files/dwug_de.zip?download=1', ''),\n",
    "                            ('dwug_en', 'English', '3.0.0', 'https://zenodo.org/records/14028531/files/dwug_en.zip?download=1', ''),\n",
    "                            ('dwug_sv', 'Swedish', '3.0.0', 'https://zenodo.org/records/14028906/files/dwug_sv.zip?download=1', ''),\n",
    "                            ('dwug_de_resampled', 'German', '1.0.0', 'https://zenodo.org/records/12670698/files/dwug_de_resampled.zip?download=1', ''),\n",
    "                            ('dwug_en_resampled', 'English', '1.0.0', 'https://zenodo.org/records/14025941/files/dwug_en_resampled.zip?download=1', ''),\n",
    "                            ('dwug_sv_resampled', 'Swedish', '1.0.0', 'https://zenodo.org/records/14026615/files/dwug_sv_resampled.zip?download=1', ''),\n",
    "                            ('discowug', 'German', '2.0.0', 'https://zenodo.org/records/14028592/files/discowug.zip?download=1', ''),\n",
    "                            ('dwug_es', 'Spanish', '4.0.2', 'https://zenodo.org/records/14891659/files/dwug_es.zip?download=1', ''),\n",
    "                            ('diawug', 'Spanish', '1.1.2', 'https://zenodo.org/records/14891461/files/diawug.zip?download=1', ''),\n",
    "                                ]\n",
    "\n",
    "if not os.path.exists('data/'):\n",
    "    os.makedirs('data/')      \n",
    "\n",
    "for name, language, version, link, path_to_data in datasets_to_download_zenodo:\n",
    "    r = requests.get(link, allow_redirects=True)\n",
    "    f = 'data/' + name + '.zip'\n",
    "    open(f, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2da647e-451a-4103-acc6-b1bda8f139d9",
   "metadata": {},
   "source": [
    "## Download data from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c3041-588e-4570-bf86-c601b5b31aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_to_download_github = [('rusemshift_1', 'Russian', '', 'https://github.com/Garrafao/RuSemShift/archive/refs/heads/correct-indices.zip', 'RuSemShift-correct-indices/rusemshift_1/DWUG/'), \n",
    "                                ('rusemshift_2', 'Russian', '', 'https://github.com/Garrafao/RuSemShift/archive/refs/heads/correct-indices.zip', 'RuSemShift-correct-indices/rusemshift_2/DWUG/'), \n",
    "                                ('rushifteval1', 'Russian', '', 'https://github.com/Garrafao/rushifteval_public/archive/refs/heads/correct-indices.zip', 'rushifteval_public-correct-indices/durel/rushifteval1/'), \n",
    "                                ('rushifteval2', 'Russian', '', 'https://github.com/Garrafao/rushifteval_public/archive/refs/heads/correct-indices.zip', 'rushifteval_public-correct-indices/durel/rushifteval2/'), \n",
    "                                ('rushifteval3', 'Russian', '', 'https://github.com/Garrafao/rushifteval_public/archive/refs/heads/correct-indices.zip', 'rushifteval_public-correct-indices/durel/rushifteval3/'), \n",
    "                                ('rudsi', 'Russian', '', 'https://github.com/Garrafao/RuDSI/archive/refs/heads/correct-indices.zip', 'RuDSI-correct-indices/'), \n",
    "                                ('nordiachange1', 'Norwegian', '', 'https://github.com/Garrafao/nor_dia_change/archive/refs/heads/correct-indices.zip', 'nor_dia_change-correct-indices/subset1/'), \n",
    "                                ('nordiachange2', 'Norwegian', '', 'https://github.com/Garrafao/nor_dia_change/archive/refs/heads/correct-indices.zip', 'nor_dia_change-correct-indices/subset2/')\n",
    "                              ]\n",
    "for name, language, version, link, path_to_data in datasets_to_download_github:\n",
    "    r = requests.get(link, allow_redirects=True)\n",
    "    f = 'data/' + name + '.zip'\n",
    "    open(f, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04002486-bfbe-4799-9640-45e9eeb65392",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_all = datasets_to_download_zenodo + datasets_to_download_github\n",
    "\n",
    "languages_global = ['German', 'English', 'Swedish', 'Spanish', 'Chinese', 'Russian', 'Norwegian']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9b9e4-2126-4d26-964b-29597ef52d0a",
   "metadata": {},
   "source": [
    "## Unzip data and remove superfluous files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da5218-aea4-4b4d-8533-1cded9328b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "for name, language, version, link, path_to_data in datasets_all:\n",
    "    if not os.path.exists('data/' + name):\n",
    "        os.makedirs('data/' + name)\n",
    "    else:\n",
    "        shutil.rmtree('data/' + name)        \n",
    "        os.makedirs('data/' + name)\n",
    "    if path_to_data == '':\n",
    "        with zipfile.ZipFile('data/' + name + '.zip') as z:\n",
    "            z.extractall('data/temp')\n",
    "        dest = shutil.move('data/temp/' + name + '/data', 'data/' + name)  \n",
    "        if os.path.exists('data/temp/' + name + '/clusters'):\n",
    "            dest = shutil.move('data/temp/' + name + '/clusters', 'data/' + name)  \n",
    "    else:\n",
    "        with zipfile.ZipFile('data/' + name + '.zip') as z:\n",
    "            z.extractall('data/temp/' + name)\n",
    "        dest = shutil.move('data/temp/' + name + '/' + path_to_data + '/data/', 'data/' + name + '/data')  \n",
    "        if os.path.exists('data/temp/' + name + '/' + path_to_data + '/clusters/'):\n",
    "            dest = shutil.move('data/temp/' + name + '/' + path_to_data + '/clusters/', 'data/' + name + '/clusters')  \n",
    "    shutil.rmtree('data/temp/' + name)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f29ecf-85db-4e31-81e1-111af637f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "# Load datasets into data frame\n",
    "df_judgments = pd.DataFrame()\n",
    "j = 0\n",
    "i2lemma2name_judgments = []\n",
    "for name, language, version, link, path_to_data in datasets_all:\n",
    "    print(name)\n",
    "    i = 0\n",
    "    for p in Path('data/'+name+'/data').glob('*/judgments.csv'):\n",
    "        #print(p)\n",
    "        lemma = str(p).split('/')[-2]        \n",
    "        lemma = unicodedata.normalize('NFC', lemma)\n",
    "        df = pd.read_csv(p, delimiter='\\t', quoting=3, na_filter=False)\n",
    "        df['dataset'] = name\n",
    "        df['language'] = language\n",
    "        df['annotator'] = df['annotator'].astype(str) + '-' + name # make sure annotators are unique across datasets\n",
    "        if name in ['chiwug']:            \n",
    "            df['identifier1'] = df['identifier1'].astype(str) + '-' + str(i) # make sure identifiers are unique across words\n",
    "            df['identifier2'] = df['identifier2'].astype(str) + '-' + str(i) # make sure identifiers are unique across words\n",
    "        if name in ['rusemshift_1', 'rusemshift_2']: # only done four judgments for those datasets which will not be mapped later\n",
    "            # don't do this for the German data where same identifiers mean same use\n",
    "            df['identifier1'] = df['identifier1'].astype(str) + '-' + str(j) # make sure identifiers are unique across datasets\n",
    "            df['identifier2'] = df['identifier2'].astype(str) + '-' + str(j) # make sure identifiers are unique across datasets\n",
    "        df['judgment'] = df['judgment'].astype(float)\n",
    "        #if df['judgment'].isnull().values.any():            \n",
    "        #    display(df[df['judgment'].isnull()])\n",
    "        df_judgments = pd.concat([df_judgments, df])\n",
    "        i2lemma2name_judgments.append((i,lemma,name))\n",
    "        i+=1\n",
    "    j+=1\n",
    "    \n",
    "df_uses = pd.DataFrame()\n",
    "j = 0\n",
    "i2lemma2name_uses = []\n",
    "for name, language, version, link, path_to_data in datasets_all:\n",
    "    i = 0\n",
    "    for p in Path('data/'+name+'/data').glob('*/uses.csv'):\n",
    "        #print(p)\n",
    "        lemma = str(p).split('/')[-2]        \n",
    "        lemma = unicodedata.normalize('NFC', lemma)\n",
    "        df = pd.read_csv(p, delimiter='\\t', quoting=3, na_filter=False)\n",
    "        df['dataset'] = name\n",
    "        df['language'] = language\n",
    "        if name in ['chiwug']:\n",
    "            df['identifier'] = df['identifier'].astype(str) + '-' + str(i) # make sure identifiers are unique across words\n",
    "            df['lemma'] = df['lemma'].apply(lambda x: unicodedata.normalize('NFC', x))\n",
    "        if name in ['rushifteval1', 'rushifteval2', 'rushifteval3', 'rusemshift_1', 'rusemshift_2']:\n",
    "            df['identifier'] = df['identifier'].astype(str) + '-' + str(j) # make sure identifiers are unique across datasets\n",
    "        df_uses = pd.concat([df_uses, df])        \n",
    "        i2lemma2name_uses.append((i,lemma,name))\n",
    "        i+=1\n",
    "    j+=1\n",
    "    \n",
    "df_clusters = pd.DataFrame()\n",
    "j = 0\n",
    "i2lemma2name_clusters = []\n",
    "for name, language, version, link, path_to_data in datasets_all:\n",
    "    i = 0    \n",
    "    for p in Path('data/'+name+'/data').glob('*/uses.csv'): # read in uses to have same order of data for clusters\n",
    "        lemma = str(p).split('/')[-2]        \n",
    "        lemma = unicodedata.normalize('NFC', lemma)\n",
    "        if os.path.exists('/'.join(str(p).split('/')[:-3] + ['clusters'])):\n",
    "            p = '/'.join(str(p).split('/')[:-3] + ['clusters/opt/'+lemma+'.csv'])\n",
    "            #print(p)\n",
    "            df = pd.read_csv(p, delimiter='\\t', quoting=3, na_filter=False)\n",
    "            df['dataset'] = name\n",
    "            df['language'] = language\n",
    "            df['lemma'] = lemma\n",
    "            if name in ['chiwug']:\n",
    "                df['identifier'] = df['identifier'].astype(str) + '-' + str(i) # make sure identifiers are unique across words\n",
    "            if name in ['rushifteval1', 'rushifteval2', 'rushifteval3', 'rusemshift_1', 'rusemshift_2']:\n",
    "                df['identifier'] = df['identifier'].astype(str) + '-' + str(j) # make sure identifiers are unique across datasets\n",
    "            df_clusters = pd.concat([df_clusters, df])        \n",
    "        i2lemma2name_clusters.append((i,lemma,name))\n",
    "        i+=1\n",
    "    j+=1        \n",
    "\n",
    "#print(i2lemma2name_judgments[:20])\n",
    "#print(i2lemma2name_uses[:20])\n",
    "#print(i2lemma2name_clusters[:20])\n",
    "#print(set(i2lemma2name_clusters).difference(set(i2lemma2name_uses)))\n",
    "assert i2lemma2name_judgments == i2lemma2name_uses == i2lemma2name_clusters\n",
    "assert not 'nan' in df_judgments['identifier1'].astype(str).unique()\n",
    "assert not 'nan' in df_judgments['identifier2'].astype(str).unique()\n",
    "df_judgments_length_before_sorting = len(df_judgments)\n",
    "df_judgments[['identifier1','identifier2']] = np.sort(df_judgments[['identifier1','identifier2']], axis=1) # sort within pairs to be able to aggregate\n",
    "assert df_judgments_length_before_sorting == len(df_judgments)\n",
    "\n",
    "display(len(df_judgments))\n",
    "display(len(df_uses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252c98d7-be6a-40a7-96b6-a5edb8b42786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample to validate\n",
    "display(df_judgments.sample(n=10))\n",
    "display(df_uses.sample(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf754c3f",
   "metadata": {},
   "source": [
    "## Clean and aggregate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c72bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 0.0 judgments with nan\n",
    "df = df_judgments.copy()\n",
    "df['judgment'] = df['judgment'].replace(0.0, np.NaN)\n",
    "\n",
    "# Aggregate use pairs and extract median column\n",
    "df = df.groupby(['identifier1', 'identifier2', 'lemma', 'dataset'])['judgment'].apply(list).reset_index(name='judgments')\n",
    "df['median_judgment'] = df['judgments'].apply(lambda x: np.nanmedian(list(x)))\n",
    "\n",
    "# Remove pairs with nan median\n",
    "#df = df[~df['median_judgment'].isnull()]\n",
    "df_judgments_aggregated = df.copy()\n",
    "display(df_judgments_aggregated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158f14f",
   "metadata": {},
   "source": [
    "## Make graphs from data and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e615462d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "nice_colors = [x for x in mcolors.get_named_colors_mapping().values() if isinstance(x, str)] # Nice colors\n",
    "colors_global = ['#377eb8', '#ff7f00', '#4daf4a', '#f781bf', '#a65628', '#984ea3', '#999999', '#e41a1c', '#dede00'] # color-blind colors\n",
    "colors_global = colors_global + nice_colors + ['#000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be81a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_graph(graph, is_weighted=False, blocks=None):\n",
    "    \n",
    "    graph = graph.copy()\n",
    "        \n",
    "    if blocks is None:        \n",
    "       blocks = [0 for node in graph.nodes()] \n",
    "    \n",
    "    fig = plt.figure(1, figsize=(12, 12), dpi=60)\n",
    "\n",
    "    if is_weighted:\n",
    "    \n",
    "        edges = graph.edges()\n",
    "        weights = [graph[i][j]['weight'] for (i,j) in graph.edges()]\n",
    "\n",
    "        # Find positions of nodes\n",
    "        graph_pos = graph.copy()\n",
    "        # edge weights are transformed for finding positions, pure weights don't reveal cluster structure in 2D\n",
    "        edges_transformed = [(i,j,graph[i][j]['weight']**5) for (i,j) in graph.edges()]\n",
    "        #print(weights_transformed)\n",
    "        graph_pos.add_weighted_edges_from(edges_transformed)\n",
    "        enan = [(u, v) for (u, v, d) in graph_pos.edges(data=True) if np.isnan(d['weight'])]        \n",
    "        graph_pos.remove_edges_from(enan)  # Remove nan edges for finding positions\n",
    "        #pos = graphviz_layout(graph_pos,prog='sfdp')   \n",
    "        pos=nx.spring_layout(graph_pos)\n",
    "        # Reduce picture size\n",
    "        pos = {node:(p[0]/2,p[1]/2) for (node,p) in pos.items()} \n",
    "\n",
    "        # Draw graph\n",
    "        nx.draw(graph, pos=pos,\n",
    "                node_size=30, \n",
    "                node_color=np.array(colors_global)[blocks],\n",
    "                edgelist=edges,\n",
    "                edge_color=weights, \n",
    "                edge_cmap=plt.cm.Greys,\n",
    "                edge_vmin=0.0,\n",
    "                edge_vmax=4.0\n",
    "               )\n",
    "    \n",
    "    else:\n",
    "        nx.draw(graph, node_size=30, node_color=np.array(colors_global)[blocks])\n",
    "        pos = {}\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return pos\n",
    "\n",
    "# Select one data set for simplicity\n",
    "df_judgments_aggregated_dataset = df_judgments_aggregated[df_judgments_aggregated['dataset'] == 'dwug_de_resampled']\n",
    "df_uses_dataset = df_uses[df_uses['dataset'] == 'dwug_de_resampled']\n",
    "df_clusters_dataset = df_clusters[df_clusters['dataset'] == 'dwug_de_resampled']\n",
    "\n",
    "# Prepare the data\n",
    "# Group by lemma\n",
    "gb = df_judgments_aggregated_dataset.groupby('lemma')    \n",
    "groups = gb.groups\n",
    "\n",
    "gb_uses = df_uses_dataset.groupby('lemma')    \n",
    "gb_clusters = df_clusters_dataset.groupby('lemma')    \n",
    "\n",
    "# construct separately for each lemma\n",
    "for word in groups.keys():\n",
    "    print(word)\n",
    "    df_group = gb.get_group(word)\n",
    "    df_uses_group = gb_uses.get_group(word)\n",
    "    df_clusters_group = gb_clusters.get_group(word)\n",
    "    clusters = df_clusters_group[df_clusters_group['identifier'].isin(df_uses_group['identifier'])]['cluster']\n",
    "    id2c = {identifier:clusters[i] for i, identifier in enumerate(df_uses_group['identifier'])}\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(df_uses_group['identifier'])\n",
    "\n",
    "    # Add edge data to graph\n",
    "    edges_weighted = list(zip(*[df_group['identifier1'], df_group['identifier2'], df_group['median_judgment']]))\n",
    "    edges = list(zip(*[df_group['identifier1'], df_group['identifier2']]))\n",
    "    graph.add_weighted_edges_from(edges_weighted)\n",
    "    \n",
    "    enan = [(u, v) for (u, v, d) in graph.edges(data=True) if np.isnan(d['weight'])] # get nan edges to remove\n",
    "    graph.remove_edges_from(enan)  # remove nan edges\n",
    "    isolates = list(nx.isolates(graph)) # get isolates to remove\n",
    "    graph.remove_nodes_from(isolates) # remove isolates\n",
    "    noise = [identifier for identifier, cluster in id2c.items() if cluster==-1] # get nodes with many 0-judgments (see DURel paper for explanation)\n",
    "    graph.remove_nodes_from(noise) # remove noise cluster\n",
    "    clusters_clean = [id2c[node] for node in graph.nodes()]\n",
    "\n",
    "    # Draw graph\n",
    "    draw_graph(graph, is_weighted=True, blocks=clusters_clean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
