{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "pzJoHIddnG8R"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9_Q71jsHVHQ",
        "outputId": "29287f55-4f9a-485c-fe37-d11fcd0ec74f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-9VfKl7AnZR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "import warnings\n",
        "\n",
        "!pip install fuzzywuzzy\n",
        "import fuzzywuzzy\n",
        "\n",
        "from fuzzywuzzy import fuzz\n",
        "import re\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xk0O_EQdFDi4"
      },
      "outputs": [],
      "source": [
        "from fuzzywuzzy import process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZDDkbconTFK"
      },
      "outputs": [],
      "source": [
        "!wget https://zenodo.org/record/7441645/files/dwug_de.zip\n",
        "with ZipFile('dwug_de.zip', 'r') as dwug_de:\n",
        "    dwug_de.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26KPir1Pq-cj"
      },
      "outputs": [],
      "source": [
        "!wget http://lcl.uniroma1.it/wsdeval/data/WSD_Unified_Evaluation_Datasets.zip\n",
        "with ZipFile('WSD_Unified_Evaluation_Datasets.zip', 'r') as dta:\n",
        "    dta.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kI-ITGecqbFo"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "tree = ET.parse('/content/WSD_Unified_Evaluation_Datasets/ALL/ALL.data.xml')\n",
        "root = tree.getroot()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemma_sentence_dict = {}\n",
        "for text in root.iter('text'):\n",
        "  #print(text.attrib) text ID\n",
        "    lemma_sentences = []\n",
        "    surface_sentences = []\n",
        "    for indd in range(len(text)): \n",
        "      sentences = []\n",
        "      if indd == 0:\n",
        "        sentences.append(text[indd])\n",
        "        sentences.append(text[indd+1]) \n",
        "      elif indd == len(text)-1:\n",
        "        sentences.append(text[indd-1])\n",
        "        sentences.append(text[indd])\n",
        "      else:\n",
        "        sentences.append(text[indd-1]) #Sentence-1 , Sentence, Sentence +1     \n",
        "        sentences.append(text[indd])\n",
        "        sentences.append(text[indd+1])\n",
        "      temp_lemma_sentences = []\n",
        "      temp_surface_sentences = []\n",
        "      index_targetword = []\n",
        "      wordid_list = []\n",
        "      pos = []\n",
        "      for sentence in sentences:\n",
        "        sentenceid = sentence.attrib['id']\n",
        "        i = 0\n",
        "        lemma_sentence = []\n",
        "        surface_sentence = []\n",
        "\n",
        "        for word in sentence:\n",
        "          lemma = word.attrib['lemma']\n",
        "          surface = word.text\n",
        "\n",
        "          if indd == 0:\n",
        "            if sentences.index(sentence) == 0:\n",
        "              p = word.attrib['pos']\n",
        "              try:\n",
        "                wordid = word.attrib['id']\n",
        "                index_targetword.append(i)\n",
        "                \n",
        "              except KeyError:\n",
        "                wordid = \"NA\"\n",
        "              wordid_list.append(wordid)\n",
        "              pos.append(p)\n",
        "          else:\n",
        "            if sentences.index(sentence) == 1:\n",
        "              p = word.attrib['pos']\n",
        "              try:\n",
        "                wordid = word.attrib['id']\n",
        "                index_targetword.append(i)\n",
        "                \n",
        "              except KeyError:\n",
        "                wordid = \"NA\"\n",
        "              wordid_list.append(wordid)\n",
        "              pos.append(p)\n",
        "          surface_sentence.append(surface)\n",
        "          lemma_sentence.append(lemma)  \n",
        "\n",
        "          i += 1\n",
        "        temp_lemma_sentences.append(lemma_sentence)\n",
        "        temp_surface_sentences.append(surface_sentence)\n",
        "        assert len(surface_sentence) == len(lemma_sentence) \n",
        "        \n",
        "\n",
        "      surface_sentences.append(temp_surface_sentences)   #[[s1, s2, s3], ]\n",
        "      lemma_sentences.append(temp_lemma_sentences)\n",
        "      #print(index_targetword)\n",
        "\n",
        "      a = 0\n",
        "      for ind in index_targetword:\n",
        "        if indd == 0:\n",
        "          try:\n",
        "            lemma_sentence_dict[temp_lemma_sentences[0][ind]].append((\n",
        "              \" \".join([' '.join(x) for x in temp_lemma_sentences]), \n",
        "              \" \".join([' '.join(x) for x in temp_surface_sentences]),\n",
        "              str(ind),\n",
        "              str(ind),\n",
        "              temp_lemma_sentences[0][ind], str(0)+':'+str(len(' '.join(temp_surface_sentences[0]))), str(0)+':'+str(len(temp_surface_sentences[0])),\n",
        "               #temp_lemma_sentences[0][ind], #str(list(nlp(\" \".join([' '.join(x) for x in temp_surface_sentences])).sents)[0]),\n",
        "              wordid_list[ind] , pos[ind],         \n",
        "              ))\n",
        "          except KeyError:\n",
        "            lemma_sentence_dict[temp_lemma_sentences[0][ind]] = [(\n",
        "              \" \".join([' '.join(x) for x in temp_lemma_sentences]), \n",
        "              \" \".join([' '.join(x) for x in temp_surface_sentences]),\n",
        "              str(ind),\n",
        "              str(ind),\n",
        "              temp_lemma_sentences[0][ind], str(0)+':'+str(len(' '.join(temp_surface_sentences[0]))), str(0)+':'+str(len(temp_surface_sentences[0])),\n",
        "               #temp_lemma_sentences[0][ind], #str(list(nlp(\" \".join([' '.join(x) for x in temp_surface_sentences])).sents)[0]),\n",
        "              wordid_list[ind] , pos[ind]         \n",
        "            )]\n",
        "        else:\n",
        "            try:\n",
        "                  lemma_sentence_dict[temp_lemma_sentences[1][ind]].append((\n",
        "                    \" \".join([' '.join(x) for x in temp_lemma_sentences]), \n",
        "                    \" \".join([' '.join(x) for x in temp_surface_sentences]),\n",
        "                    str(ind),\n",
        "                    str(ind),\n",
        "                    temp_lemma_sentences[1][ind], str(len(' '.join(temp_surface_sentences[0])))+':'+str(len(' '.join(temp_surface_sentences[0]))+len(' '.join(temp_surface_sentences[1]))+1),\n",
        "                    str(len((temp_surface_sentences[0])))+':'+str(len((temp_surface_sentences[0]))+len((temp_surface_sentences[1]))),\n",
        "                    #temp_lemma_sentences[1][ind],# str((\" \".join([' '.join(x) for x in temp_surface_sentences]))),\n",
        "                    wordid_list[ind] , pos[ind]       \n",
        "                    ))\n",
        "            except KeyError:\n",
        "                  lemma_sentence_dict[temp_lemma_sentences[1][ind]] = [(\n",
        "                    \" \".join([' '.join(x) for x in temp_lemma_sentences]), \n",
        "                    \" \".join([' '.join(x) for x in temp_surface_sentences]),\n",
        "                    str(ind),\n",
        "                    str(ind),\n",
        "                    temp_lemma_sentences[1][ind], str(len(' '.join(temp_surface_sentences[0])))+':'+str(len(' '.join(temp_surface_sentences[0]))+len(' '.join(temp_surface_sentences[1]))+1), \n",
        "                    str(len((temp_surface_sentences[0])))+':'+str(len((temp_surface_sentences[0]))+len((temp_surface_sentences[1]))),\n",
        "                    #temp_lemma_sentences[1][ind], #str(0)+ ':'+str((\" \".join([' '.join(x) for x in temp_surface_sentences]))),\n",
        "                    wordid_list[ind] , pos[ind]         \n",
        "                  )]\n",
        "        \n",
        "   "
      ],
      "metadata": {
        "id": "hp43f4mUL2qs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ujKsIpGfqKoH"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"data_files\"):\n",
        "  os.mkdir(\"data_files\")\n",
        "\n",
        "with open(f\"data_files/all.tsv\", \"w\") as f:\n",
        "  for lemma in lemma_sentence_dict:\n",
        "    #f.write(lemma +\"\\n\")\n",
        "    for x in lemma_sentence_dict[lemma]:\n",
        "      to_write =  \"\\t\".join(x)+\"\\n\" \n",
        "      f.write(to_write)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xXt5U2HqdAuQ"
      },
      "outputs": [],
      "source": [
        "def getname(dataset):\n",
        "  return dataset.split('.')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PaNUJhaDZEkS"
      },
      "outputs": [],
      "source": [
        "wsd = pd.read_csv('/content/data_files/all.tsv', delimiter = '\\t', names = ['sent1','sent2', 'index1', 'index2', 'lemma', 'l1', 'indexes_target_sentence_tokenized','identifier', 'pos'], quoting = 3)\n",
        "wsd['dataset'] = wsd['identifier'].apply(getname)\n",
        "wsd = wsd[['sent2', 'lemma', 'l1', 'indexes_target_sentence_tokenized', 'identifier', 'pos', 'dataset', 'index2']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Nq7ChJ6VAmM1"
      },
      "outputs": [],
      "source": [
        "wsd.rename(columns = {'sent2':'context_tokenized'}, inplace = True)\n",
        "wsd.rename(columns = {'l1':'indexes_target_sentence'}, inplace = True)\n",
        "wsd['context'] = wsd['context_tokenized']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "sLvpFGhLbSu1"
      },
      "outputs": [],
      "source": [
        "def get_target(sentence, word):\n",
        "    grams = re.split(r\"[^a-zA-Z0-9]\", word)\n",
        "    tok = re.split(r\"[^a-zA-Z0-9]\", sentence)\n",
        "    big = list(ngrams(tok, len(grams)))\n",
        "    #print(big)\n",
        "    match_ratio = 0\n",
        "    gram = \"\"\n",
        "    gram1 = \"\"\n",
        "    idx = -1\n",
        "    for index, i in enumerate(big):\n",
        "        ratio = fuzz.token_sort_ratio(\" \".join(i).lower(), \" \".join(grams).lower())\n",
        "        if ratio>match_ratio:\n",
        "            match_ratio = ratio\n",
        "            gram = \" \".join(i).lower()\n",
        "            gram1 = \" \".join(grams).lower()\n",
        "            idx = index\n",
        "    #print(gram, gram1, match_ratio, idx)\n",
        "    length = len(gram)\n",
        "    length1 = len(\" \".join(tok[:idx]))+1\n",
        "    targ_token = sentence[length1:length1+length]\n",
        "    print(sentence[length1:length1+length], length1, idx)\n",
        "    if len(targ_token.split(\" \"))>1:\n",
        "      return str(idx) + ':' + str(idx +len(targ_token.split(\" \")))\n",
        "    else:\n",
        "      return idx\n",
        "    \n",
        "    #if"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "R8Z5gPp2Lp2C"
      },
      "outputs": [],
      "source": [
        "def get_indice(sentence, word):\n",
        "    grams = re.split(r\"[^a-zA-Z0-9]\", word)\n",
        "    tok = re.split(r\"[^a-zA-Z0-9]\", sentence)\n",
        "    big = list(ngrams(tok, len(grams)))\n",
        "    #print(big)\n",
        "    match_ratio = 0\n",
        "    gram = \"\"\n",
        "    gram1 = \"\"\n",
        "    idx = -1\n",
        "    for index, i in enumerate(big):\n",
        "        ratio = fuzz.token_sort_ratio(\" \".join(i).lower(), \" \".join(grams).lower())\n",
        "        if ratio>match_ratio:\n",
        "            match_ratio = ratio\n",
        "            gram = \" \".join(i).lower()\n",
        "            gram1 = \" \".join(grams).lower()\n",
        "            idx = index\n",
        "    #print(gram, gram1, match_ratio, idx)\n",
        "    length = len(gram)\n",
        "    length1 = len(\" \".join(tok[:idx]))+1\n",
        "    targ_token = sentence[length1:length1+length]\n",
        "    target_indice = str(length1) + ':' + str((length1) + (length))\n",
        "    return target_indice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "AhfG8XxLdGAU"
      },
      "outputs": [],
      "source": [
        "wsd = wsd.reset_index(drop= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ApXz3BogdXvp"
      },
      "outputs": [],
      "source": [
        "wsd['indexes_target_token_tokenized'] = wsd.apply(lambda x: get_target(x.context_tokenized, x.lemma), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "fHEMpPHDLzXN"
      },
      "outputs": [],
      "source": [
        "wsd['indexes_target_token'] = wsd.apply(lambda x: get_indice(x.context_tokenized, x.lemma), axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "vfHh4OqKwBTB"
      },
      "outputs": [],
      "source": [
        "wsd['grouping'] = 1\n",
        "wsd['date'] =  ' '\n",
        "wsd['description'] = ' '\n",
        "wsd = wsd[['lemma', 'pos', 'date', 'grouping','identifier', 'description', 'context', 'indexes_target_token', 'indexes_target_sentence', 'context_tokenized', 'indexes_target_token_tokenized', 'indexes_target_sentence_tokenized' ,'dataset']]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ipKE66dXOBGz"
      },
      "outputs": [],
      "source": [
        "uses = wsd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KdYd1xWWhXN"
      },
      "outputs": [],
      "source": [
        "uses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#need to modify the use2normalize script for token spans in wsd"
      ],
      "metadata": {
        "id": "QYbO2rwPjYpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIQSzOEUhjKI"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Garrafao/WUGs.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-awSjmCN9DQ"
      },
      "outputs": [],
      "source": [
        "os.makedirs('/content/WUGs/scripts/misc/data/dwug_en/data/*/', exist_ok=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPfj8EKviySE"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "uses.to_csv('/content/WUGs/scripts/misc/data/dwug_en/data/*/uses.csv', sep='\\t', encoding='utf-8', quoting = csv.QUOTE_NONE, quotechar='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GE3AolIlCm-"
      },
      "outputs": [],
      "source": [
        "!cd /content/WUGs/scripts/misc && bash -e /content/WUGs/scripts/misc/use2normalize.sh "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTQlBpnZFh9I"
      },
      "outputs": [],
      "source": [
        "%run /content/WUGs/scripts/misc/use2normalize.py /content/WUGs/scripts/misc/data/dwug_en/data/*/uses.csv dwug_en /content/WUGs/scripts/misc/data/dwug_en/data/*/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cd53WUqantvS"
      },
      "outputs": [],
      "source": [
        "final_uses = pd.read_csv('/content/WUGs/scripts/misc/data/dwug_en/use_data/normalize/*/uses.csv', delimiter = '\\t',quoting =3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn7dXXXaVtwi"
      },
      "outputs": [],
      "source": [
        "final_uses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1T-KH8Q57fQ"
      },
      "outputs": [],
      "source": [
        "wsd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9k0QVouZWtG1"
      },
      "outputs": [],
      "source": [
        "final_uses['context_tokenized'] = wsd['context_tokenized']\n",
        "final_uses['pos'] = wsd['pos']\n",
        "final_uses['dataset'] = wsd['dataset_name']\n",
        "final_uses['grouping'] = wsd['grouping']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cs9YeRVJmSA0"
      },
      "outputs": [],
      "source": [
        "labels = pd.read_csv(\"/content/WSD_Unified_Evaluation_Datasets/ALL/ALL.gold.key.txt\", delimiter = '\\t', names = ['identifier', 'identifier_sense'])\n",
        "labels['identifier_sense'] = labels['identifier'].apply(lambda x: tok(x)[1])\n",
        "labels['identifier'] = labels['identifier'].apply(lambda x: tok(x)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVK2JlEQmMJo"
      },
      "outputs": [],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaiikG6DANhp"
      },
      "outputs": [],
      "source": [
        "dirlist = os.listdir('dwug_de/misc/dwug_de_sense/data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ufn5kxLIARBn"
      },
      "outputs": [],
      "source": [
        "lis_uses = []\n",
        "for i in dirlist:\n",
        "  lis_uses.append(\"dwug_de/misc/dwug_de_sense/data/\" + i + \"/uses.csv\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-8o6B7Ykr2v"
      },
      "outputs": [],
      "source": [
        "lis_judgments = []\n",
        "for i in dirlist:\n",
        "  lis_judgments.append(\"dwug_de/misc/dwug_de_sense/data/\" + i + \"/maj_3/judgments.csv\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB4cCkd-kx3F"
      },
      "outputs": [],
      "source": [
        "lis_senses = []\n",
        "for i in dirlist:\n",
        "  lis_senses.append(\"dwug_de/misc/dwug_de_sense/data/\" + i + \"/senses.csv\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvTsiPMBlPWM"
      },
      "outputs": [],
      "source": [
        "dwugde_senses = pd.DataFrame()             \n",
        "for i in lis_senses:\n",
        "   Tmp = pd.read_csv(i, delimiter='\\t', quoting = 3)\n",
        "   Tmp['dataset_name'] = i.split('/')[2]\n",
        "   dwugde_senses = pd.concat([dwugde_senses, Tmp])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_JLITrKlB5b"
      },
      "outputs": [],
      "source": [
        "dwugdesense_gold = pd.DataFrame()             \n",
        "for i in lis_judgments:\n",
        "   Tmp = pd.read_csv(i, delimiter='\\t', quoting = 3)\n",
        "   Tmp['dataset_name'] = i.split('/')[2]\n",
        "   dwugdesense_gold = pd.concat([dwugdesense_gold, Tmp])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwFSQgBBAUiG"
      },
      "outputs": [],
      "source": [
        "dwugde_uses = pd.DataFrame()             \n",
        "for i in lis_uses:\n",
        "   Tmp = pd.read_csv(i, delimiter='\\t', quoting = 3)\n",
        "   Tmp['dataset_name'] = i.split('/')[2]\n",
        "   dwugde_uses = pd.concat([dwugde_uses, Tmp])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFxYpSB0Al-s"
      },
      "outputs": [],
      "source": [
        "dwugde_uses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1Q9hZsylqbJ"
      },
      "outputs": [],
      "source": [
        "dwugde_senses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUBN4F_DHFe6"
      },
      "outputs": [],
      "source": [
        "dwugdesense_gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6v8e6aMIlwav"
      },
      "outputs": [],
      "source": [
        "#dwugde_judgments #should be renamed as gold and should take from maj3 folder from every lemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlWJ357HHOnt"
      },
      "outputs": [],
      "source": [
        "wsd_final_uses = pd.DataFrame()\n",
        "wsd_final_uses = pd.concat([wsd,dwugde_uses], axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMwxpmYXH6NE"
      },
      "outputs": [],
      "source": [
        "wsd_final_uses = wsd_final_uses.reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVDkjp1YBXjN"
      },
      "outputs": [],
      "source": [
        "#dwug_de_sense, WSsim, semseval, semeval2010/2012, etc "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}