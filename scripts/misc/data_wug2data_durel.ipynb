{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e76464b9-6979-45ea-80e0-edb634ca0bae",
   "metadata": {},
   "source": [
    "## Download data from Zenodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d675b9b-cc4d-45be-bf0c-934067019882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "datasets_to_download_zenodo = [('refwug', 'German', '1.1.0', 'https://zenodo.org/records/5791269/files/refwug.zip?download=1', ''), \n",
    "                            ('durel', 'German', '3.0.0', 'https://zenodo.org/records/5784453/files/durel.zip?download=1', ''),\n",
    "                            ('surel', 'German', '3.0.0', 'https://zenodo.org/records/5784569/files/surel.zip?download=1', ''), \n",
    "                            ('chiwug', 'Chinese', '1.0.0', 'https://zenodo.org/records/10023263/files/chiwug.zip?download=1', ''),\n",
    "                            ('dwug_de', 'German', '3.0.0', 'https://zenodo.org/records/14028509/files/dwug_de.zip?download=1', ''),\n",
    "                            ('dwug_en', 'English', '3.0.0', 'https://zenodo.org/records/14028531/files/dwug_en.zip?download=1', ''),\n",
    "                            ('dwug_sv', 'Swedish', '3.0.0', 'https://zenodo.org/records/14028906/files/dwug_sv.zip?download=1', ''),\n",
    "                            ('dwug_de_resampled', 'German', '1.0.0', 'https://zenodo.org/records/12670698/files/dwug_de_resampled.zip?download=1', ''),\n",
    "                            ('dwug_en_resampled', 'English', '1.0.0', 'https://zenodo.org/records/14025941/files/dwug_en_resampled.zip?download=1', ''),\n",
    "                            ('dwug_sv_resampled', 'Swedish', '1.0.0', 'https://zenodo.org/records/14026615/files/dwug_sv_resampled.zip?download=1', ''),\n",
    "                            ('discowug', 'German', '2.0.0', 'https://zenodo.org/records/14028592/files/discowug.zip?download=1', ''),\n",
    "                            ('dwug_es', 'Spanish', '4.0.2', 'https://zenodo.org/records/14891659/files/dwug_es.zip?download=1', ''),\n",
    "                            ('diawug', 'Spanish', '1.1.2', 'https://zenodo.org/records/14891461/files/diawug.zip?download=1', ''),\n",
    "                                ]\n",
    "\n",
    "if not os.path.exists('data/'):\n",
    "    os.makedirs('data/')      \n",
    "\n",
    "for name, language, version, link, path_to_data in datasets_to_download_zenodo:\n",
    "    r = requests.get(link, allow_redirects=True)\n",
    "    f = 'data/' + name + '.zip'\n",
    "    open(f, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2da647e-451a-4103-acc6-b1bda8f139d9",
   "metadata": {},
   "source": [
    "## Download data from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c3041-588e-4570-bf86-c601b5b31aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "datasets_to_download_github = [('rusemshift_1', 'Russian', '', 'https://github.com/Garrafao/RuSemShift/archive/refs/heads/correct-indices.zip', 'RuSemShift-correct-indices/rusemshift_1/DWUG/'), \n",
    "                                ('rusemshift_2', 'Russian', '', 'https://github.com/Garrafao/RuSemShift/archive/refs/heads/correct-indices.zip', 'RuSemShift-correct-indices/rusemshift_2/DWUG/'), \n",
    "                                ('rushifteval1', 'Russian', '', 'https://github.com/Garrafao/rushifteval_public/archive/refs/heads/correct-indices.zip', 'rushifteval_public-correct-indices/durel/rushifteval1/'), \n",
    "                                ('rushifteval2', 'Russian', '', 'https://github.com/Garrafao/rushifteval_public/archive/refs/heads/correct-indices.zip', 'rushifteval_public-correct-indices/durel/rushifteval2/'), \n",
    "                                ('rushifteval3', 'Russian', '', 'https://github.com/Garrafao/rushifteval_public/archive/refs/heads/correct-indices.zip', 'rushifteval_public-correct-indices/durel/rushifteval3/'), \n",
    "                                ('rudsi', 'Russian', '', 'https://github.com/Garrafao/RuDSI/archive/refs/heads/correct-indices.zip', 'RuDSI-correct-indices/'), \n",
    "                                ('nordiachange1', 'Norwegian', '', 'https://github.com/Garrafao/nor_dia_change/archive/refs/heads/correct-indices.zip', 'nor_dia_change-correct-indices/subset1/'), \n",
    "                                ('nordiachange2', 'Norwegian', '', 'https://github.com/Garrafao/nor_dia_change/archive/refs/heads/correct-indices.zip', 'nor_dia_change-correct-indices/subset2/')\n",
    "                              ]\n",
    "for name, language, version, link, path_to_data in datasets_to_download_github:\n",
    "    r = requests.get(link, allow_redirects=True)\n",
    "    f = 'data/' + name + '.zip'\n",
    "    open(f, 'wb').write(r.content)\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04002486-bfbe-4799-9640-45e9eeb65392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndatasets_all = datasets_to_download_zenodo + datasets_to_download_github\\n\\nlanguages_global = ['German', 'English', 'Swedish', 'Spanish', 'Chinese', 'Russian', 'Norwegian']\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_all = datasets_to_download_zenodo\n",
    "\n",
    "'''\n",
    "datasets_all = datasets_to_download_zenodo + datasets_to_download_github\n",
    "\n",
    "languages_global = ['German', 'English', 'Swedish', 'Spanish', 'Chinese', 'Russian', 'Norwegian']\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9b9e4-2126-4d26-964b-29597ef52d0a",
   "metadata": {},
   "source": [
    "## Unzip data and remove superfluous files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8da5218-aea4-4b4d-8533-1cded9328b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "for name, language, version, link, path_to_data in datasets_all:\n",
    "    if not os.path.exists('data/' + name):\n",
    "        os.makedirs('data/' + name)\n",
    "    else:\n",
    "        shutil.rmtree('data/' + name)        \n",
    "        os.makedirs('data/' + name)\n",
    "    if path_to_data == '':\n",
    "        with zipfile.ZipFile('data/' + name + '.zip') as z:\n",
    "            z.extractall('data/temp')\n",
    "        dest = shutil.move('data/temp/' + name + '/data', 'data/' + name)  \n",
    "        if os.path.exists('data/temp/' + name + '/clusters'):\n",
    "            dest = shutil.move('data/temp/' + name + '/clusters', 'data/' + name)  \n",
    "    else:\n",
    "        with zipfile.ZipFile('data/' + name + '.zip') as z:\n",
    "            z.extractall('data/temp/' + name)\n",
    "        dest = shutil.move('data/temp/' + name + '/' + path_to_data + '/data/', 'data/' + name + '/data')  \n",
    "        if os.path.exists('data/temp/' + name + '/' + path_to_data + '/clusters/'):\n",
    "            dest = shutil.move('data/temp/' + name + '/' + path_to_data + '/clusters/', 'data/' + name + '/clusters')  \n",
    "    shutil.rmtree('data/temp/' + name)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73f29ecf-85db-4e31-81e1-111af637f8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    \\ndf_clusters = pd.DataFrame()\\nj = 0\\ni2lemma2name_clusters = []\\nfor name, language, version, link, path_to_data in datasets_all:\\n    i = 0    \\n    for p in Path('data/'+name+'/data').glob('*/uses.csv'): # read in uses to have same order of data for clusters\\n        lemma = str(p).split('/')[-2]        \\n        lemma = unicodedata.normalize('NFC', lemma)\\n        if os.path.exists('/'.join(str(p).split('/')[:-3] + ['clusters'])):\\n            p = '/'.join(str(p).split('/')[:-3] + ['clusters/opt/'+lemma+'.csv'])\\n            #print(p)\\n            df = pd.read_csv(p, delimiter='\\t', quoting=3, na_filter=False)\\n            df['dataset'] = name\\n            df['language'] = language\\n            df['lemma'] = lemma\\n            if name in ['chiwug']:\\n                df['identifier'] = df['identifier'].astype(str) + '-' + str(i) # make sure identifiers are unique across words\\n            if name in ['rushifteval1', 'rushifteval2', 'rushifteval3', 'rusemshift_1', 'rusemshift_2']:\\n                df['identifier'] = df['identifier'].astype(str) + '-' + str(j) # make sure identifiers are unique across datasets\\n            df_clusters = pd.concat([df_clusters, df])        \\n        i2lemma2name_clusters.append((i,lemma,name))\\n        i+=1\\n    j+=1        \\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "# Load datasets into data frame\n",
    "df_judgments = pd.DataFrame()\n",
    "j = 0\n",
    "i2lemma2name_judgments = []\n",
    "for name, language, version, link, path_to_data in datasets_all:\n",
    "    print(name)\n",
    "    i = 0\n",
    "    for p in Path('data/'+name+'/data').glob('*/judgments.csv'):\n",
    "        #print(p)\n",
    "        lemma = str(p).split('/')[-2]        \n",
    "        lemma = unicodedata.normalize('NFC', lemma)\n",
    "        df = pd.read_csv(p, delimiter='\\t', quoting=3, na_filter=False)\n",
    "        df['dataset'] = name\n",
    "        df['language'] = language\n",
    "        df['annotator'] = df['annotator'].astype(str) + '-' + name # make sure annotators are unique across datasets\n",
    "        if name in ['chiwug']:            \n",
    "            df['identifier1'] = df['identifier1'].astype(str) + '-' + str(i) # make sure identifiers are unique across words\n",
    "            df['identifier2'] = df['identifier2'].astype(str) + '-' + str(i) # make sure identifiers are unique across words\n",
    "        if name in ['rusemshift_1', 'rusemshift_2']: # only done four judgments for those datasets which will not be mapped later\n",
    "            # don't do this for the German data where same identifiers mean same use\n",
    "            df['identifier1'] = df['identifier1'].astype(str) + '-' + str(j) # make sure identifiers are unique across datasets\n",
    "            df['identifier2'] = df['identifier2'].astype(str) + '-' + str(j) # make sure identifiers are unique across datasets\n",
    "        df['judgment'] = df['judgment'].astype(float)\n",
    "        #if df['judgment'].isnull().values.any():            \n",
    "        #    display(df[df['judgment'].isnull()])\n",
    "        df_judgments = pd.concat([df_judgments, df])\n",
    "        i2lemma2name_judgments.append((i,lemma,name))\n",
    "        i+=1\n",
    "    j+=1\n",
    "'''    \n",
    "df_uses = pd.DataFrame()\n",
    "j = 0\n",
    "i2lemma2name_uses = []\n",
    "for name, language, version, link, path_to_data in datasets_all:\n",
    "    if not os.path.exists('data/' + name +'/data_durel'):\n",
    "        os.makedirs('data/' + name +'/data_durel')\n",
    "    i = 0\n",
    "    for p in Path('data/'+name+'/data').glob('*/uses.csv'):\n",
    "        #print(p)\n",
    "        lemma = str(p).split('/')[-2]        \n",
    "        lemma = unicodedata.normalize('NFC', lemma)\n",
    "        shutil.copyfile(p, 'data/' + name +'/data_durel/' + lemma + '_uses.csv')                \n",
    "        continue\n",
    "        df = pd.read_csv(p, delimiter='\\t', quoting=3, na_filter=False)\n",
    "        df['dataset'] = name\n",
    "        df['language'] = language\n",
    "        if name in ['chiwug']:\n",
    "            df['identifier'] = df['identifier'].astype(str) + '-' + str(i) # make sure identifiers are unique across words\n",
    "            df['lemma'] = df['lemma'].apply(lambda x: unicodedata.normalize('NFC', x))\n",
    "        if name in ['rushifteval1', 'rushifteval2', 'rushifteval3', 'rusemshift_1', 'rusemshift_2']:\n",
    "            df['identifier'] = df['identifier'].astype(str) + '-' + str(j) # make sure identifiers are unique across datasets\n",
    "        df_uses = pd.concat([df_uses, df])        \n",
    "        i2lemma2name_uses.append((i,lemma,name))\n",
    "        i+=1\n",
    "    j+=1\n",
    "'''    \n",
    "df_clusters = pd.DataFrame()\n",
    "j = 0\n",
    "i2lemma2name_clusters = []\n",
    "for name, language, version, link, path_to_data in datasets_all:\n",
    "    i = 0    \n",
    "    for p in Path('data/'+name+'/data').glob('*/uses.csv'): # read in uses to have same order of data for clusters\n",
    "        lemma = str(p).split('/')[-2]        \n",
    "        lemma = unicodedata.normalize('NFC', lemma)\n",
    "        if os.path.exists('/'.join(str(p).split('/')[:-3] + ['clusters'])):\n",
    "            p = '/'.join(str(p).split('/')[:-3] + ['clusters/opt/'+lemma+'.csv'])\n",
    "            #print(p)\n",
    "            df = pd.read_csv(p, delimiter='\\t', quoting=3, na_filter=False)\n",
    "            df['dataset'] = name\n",
    "            df['language'] = language\n",
    "            df['lemma'] = lemma\n",
    "            if name in ['chiwug']:\n",
    "                df['identifier'] = df['identifier'].astype(str) + '-' + str(i) # make sure identifiers are unique across words\n",
    "            if name in ['rushifteval1', 'rushifteval2', 'rushifteval3', 'rusemshift_1', 'rusemshift_2']:\n",
    "                df['identifier'] = df['identifier'].astype(str) + '-' + str(j) # make sure identifiers are unique across datasets\n",
    "            df_clusters = pd.concat([df_clusters, df])        \n",
    "        i2lemma2name_clusters.append((i,lemma,name))\n",
    "        i+=1\n",
    "    j+=1        \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1744852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
